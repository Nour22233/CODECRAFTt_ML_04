# -*- coding: utf-8 -*-
"""codecraftml task4.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Obn2PssrDKTS8WBakO6cvQ5qXZ79iMBS
"""

pip install opencv-python mediapipe scikit-learn numpy

pip install tensorflow

import numpy
print(numpy.__version__)

import os
import cv2
import numpy as np
import matplotlib.pyplot as plt
import shutil
import warnings
from sklearn.model_selection import train_test_split
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense
from tensorflow.keras.utils import to_categorical
from sklearn.preprocessing import LabelEncoder
from cv2 import imread, resize, IMREAD_GRAYSCALE
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay
from tensorflow import nn
from keras import models, layers, regularizers, optimizers, callbacks

warnings.filterwarnings('ignore')

data_path = "."



X, y = [], []


for folder in os.listdir(data_path):
    folder_path = os.path.join(data_path, folder)
    if not os.path.isdir(folder_path):
        continue

    for img_file in os.listdir(folder_path):
        if img_file.endswith(".png"):
            img_path = os.path.join(folder_path, img_file)

            img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)
            img = cv2.resize(img, (64, 64))

            X.append(img)
            y.append(folder)


X = np.array(X).reshape(-1, 64, 64, 1) / 255.0
y = np.array(y)

print("âœ… Ø¹Ø¯Ø¯ Ø§Ù„ØµÙˆØ±:", len(X))
print("âœ… Ø´ÙƒÙ„ X:", X.shape)
print("âœ… Ø¹Ø¯Ø¯ Ø§Ù„ÙØ¦Ø§Øª:", len(np.unique(y)))

import os

data_path = r"C:\Users\PointofSale\Downloads\archive\leapGestRecog"

print("Ø§Ù„Ù…Ø¬Ù„Ø¯Ø§Øª Ø§Ù„Ù…ÙˆØ¬ÙˆØ¯Ø©:")
print(os.listdir(data_path))

import os
import cv2
import numpy as np

data_dir = r"C:\Users\PointofSale\Downloads\archive\leapGestRecog"

X = []
y = []
img_size = 64


for person in os.listdir(data_dir):
    person_path = os.path.join(data_dir, person)
    if not os.path.isdir(person_path):
        continue

    print(f"Ø¬Ø§Ø±Ù Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„ÙØ¦Ø©: {person}")

    for class_name in os.listdir(person_path):
        class_path = os.path.join(person_path, class_name)
        if not os.path.isdir(class_path):
            continue

        for img_name in os.listdir(class_path):
            img_path = os.path.join(class_path, img_name)
            try:
                img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)
                img = cv2.resize(img, (img_size, img_size))
                X.append(img)
                y.append(class_name)
            except Exception as e:
                print(f"Ø®Ø·Ø£ ÙÙŠ ØªØ­Ù…ÙŠÙ„ Ø§Ù„ØµÙˆØ±Ø© {img_path}: {e}")

X = np.array(X).reshape(-1, img_size, img_size, 1)
y = np.array(y)

print("âœ… Ø¹Ø¯Ø¯ Ø§Ù„ØµÙˆØ±:", len(X))
print("âœ… Ø´ÙƒÙ„ X:", X.shape)
print("âœ… Ø¹Ø¯Ø¯ Ø§Ù„ÙØ¦Ø§Øª:", len(set(y)))

import numpy as np
from sklearn.model_selection import train_test_split


X = np.array(X).reshape(-1, 64, 64, 1)
y = np.array(y)


X = X / 255.0


X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

print(f"ðŸŸ¢ X_train: {X_train.shape}, y_train: {y_train.shape}")
print(f"ðŸ”µ X_test: {X_test.shape}, y_test: {y_test.shape}")

import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout

model = Sequential([
    Conv2D(32, (3, 3), activation='relu', input_shape=(64, 64, 1)),
    MaxPooling2D(pool_size=(2, 2)),

    Conv2D(64, (3, 3), activation='relu'),
    MaxPooling2D(pool_size=(2, 2)),

    Flatten(),
    Dense(128, activation='relu'),
    Dropout(0.5),
    Dense(len(np.unique(y)), activation='softmax')
])

model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])

model.summary()

from sklearn.preprocessing import LabelEncoder
from tensorflow.keras.utils import to_categorical

le = LabelEncoder()
y_train_encoded = le.fit_transform(y_train)
y_test_encoded = le.transform(y_test)


y_train_cat = to_categorical(y_train_encoded, num_classes=10)
y_test_cat = to_categorical(y_test_encoded, num_classes=10)

data_path = r"C:\Users\PointofSale\Downloads\archive\leapGestRecog"

base_user = os.path.join(data_path, "00")
gesture_classes = os.listdir(base_user)

gesture_folders = [os.path.join(base_user, g) for g in gesture_classes if os.path.isdir(os.path.join(base_user, g))]

for folder in gesture_folders:
    for image_name in os.listdir(folder):
        image_path = os.path.join(folder, image_name)
        img_array = cv2.imread(image_path, 0)  # 0 = gray
        if img_array is not None:
            plt.imshow(img_array, cmap='gray')
            plt.title(f"First image from gesture: {os.path.basename(folder)}")
            plt.axis('off')
            plt.show()
        break

from keras.utils import to_categorical
from sklearn.preprocessing import LabelEncoder


label_encoder = LabelEncoder()
y_train_encoded = label_encoder.fit_transform(y_train)
y_test_encoded = label_encoder.transform(y_test)


num_classes = len(label_encoder.classes_)
y_train_cat = to_categorical(y_train_encoded, num_classes=num_classes)
y_test_cat = to_categorical(y_test_encoded, num_classes=num_classes)


from keras.models import Sequential
from keras.layers import Dense, Flatten

model = Sequential()
model.add(Flatten(input_shape=X_train.shape[1:]))
model.add(Dense(128, activation='relu'))
model.add(Dense(num_classes, activation='softmax'))


model.compile(optimizer='adam',
              loss='categorical_crossentropy',
              metrics=['accuracy'])


history = model.fit(X_train, y_train_cat,
                    epochs=10,
                    validation_data=(X_test, y_test_cat))

import matplotlib.pyplot as plt


plt.plot(history.history['accuracy'], label='Training Accuracy')
plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
plt.title('Model Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()
plt.show()


plt.plot(history.history['loss'], label='Training Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.title('Model Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()
plt.show()

model.save("gesture_cnn_model.h5")
print("Model saved successfully.")

from tensorflow.keras.models import load_model


model = load_model("gesture_cnn_model.h5")
print("Model loaded successfully.")

import cv2
import numpy as np


img_path = r"C:\Users\PointofSale\Downloads\archive\leapGestRecog\00\01_palm\frame_00_01_0001.png"


img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)
img = cv2.resize(img, (64, 64))
img = img.astype('float32') / 255.0
img = np.expand_dims(img, axis=-1)
img = np.expand_dims(img, axis=0)

pred = model.predict(img)
predicted_label = np.argmax(pred)
print("Predicted class:", predicted_label)

img_path = r"C:\Users\PointofSale\Downloads\archive\leapGestRecog\00\01_palm\frame_00_01_0001.png"


img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)
img = cv2.resize(img, (64, 64))
img = img.astype('float32') / 255.0
img = np.expand_dims(img, axis=-1)
img = np.expand_dims(img, axis=0)


gesture_labels = [
    "Palm", "L", "Fist", "Fist_moved", "Thumb",
    "Index", "Ok", "Palm_moved", "C", "Down"
]


pred = model.predict(img)
predicted_label = np.argmax(pred)
gesture_name = gesture_labels[predicted_label]

print("Predicted class index:", predicted_label)
print("Predicted gesture:", gesture_name)

pip install pyautogui

import cv2
import numpy as np
from tensorflow.keras.models import load_model


model = load_model('gesture_cnn_model.h5')


gesture_classes = ['Palm', 'L', 'Fist', 'Fist_moved', 'Thumb', 'Index', 'Ok', 'Palm_moved', 'C', 'Down']


IMG_WIDTH = 64
IMG_HEIGHT = 64


cap = cv2.VideoCapture(0)

while True:
    ret, frame = cap.read()
    if not ret:
        break

    x1, y1, x2, y2 = 300, 100, 600, 400
    roi = frame[y1:y2, x1:x2]


    cv2.rectangle(frame, (x1, y1), (x2, y2), (0,255,0), 2)

    gray = cv2.cvtColor(roi, cv2.COLOR_BGR2GRAY)
    blur = cv2.GaussianBlur(gray, (5,5), 2)
    _, thresh = cv2.threshold(blur, 70, 255, cv2.THRESH_BINARY_INV+cv2.THRESH_OTSU)

    resized = cv2.resize(thresh, (IMG_WIDTH, IMG_HEIGHT))
    normalized = resized.astype('float32') / 255.0
    reshaped = normalized.reshape(1, IMG_WIDTH, IMG_HEIGHT, 1)


    prediction = model.predict(reshaped)
    class_id = np.argmax(prediction)
    gesture_name = gesture_classes[class_id]


    cv2.putText(frame, gesture_name, (x1, y1-10), cv2.FONT_HERSHEY_SIMPLEX, 1, (0,0,255), 2)


    cv2.imshow("Webcam", frame)
    cv2.imshow("ROI", thresh)

    key = cv2.waitKey(1)
    if key == 27:
        break

cap.release()
cv2.destroyAllWindows()

import cv2
import numpy as np
import time
import subprocess
from tensorflow.keras.models import load_model


model = load_model('gesture_cnn_model.h5')


gesture_classes = ['Palm', 'L', 'Fist', 'Fist_moved', 'Thumb', 'Index', 'Ok', 'Palm_moved', 'C', 'Down']


gesture_actions = {
    'Palm': lambda: subprocess.Popen(['calc']),
    'L': lambda: subprocess.Popen(['notepad']),
    'Fist': lambda: subprocess.Popen(['cmd', '/c', 'start https://www.google.com']),
    'Fist_moved': lambda: subprocess.Popen(['cmd', '/c', 'start https://www.youtube.com']),
    'Thumb': lambda: subprocess.call("nircmd.exe mutesysvolume 0"),
    'Index': lambda: subprocess.call("nircmd.exe mutesysvolume 1"),
    'Ok': lambda: subprocess.call("nircmd.exe savescreenshot screenshot.png"),
    'Palm_moved': lambda: subprocess.call("taskkill /f /im chrome.exe"),
    'C': lambda: print("No action assigned to 'C'"),
    'Down': lambda: print("No action assigned to 'Down'")
}

IMG_WIDTH = 64
IMG_HEIGHT = 64

cap = cv2.VideoCapture(0)
last_executed = None
cooldown = 2
last_time = time.time()

while True:
    ret, frame = cap.read()
    if not ret:
        break

    x1, y1, x2, y2 = 300, 100, 600, 400
    roi = frame[y1:y2, x1:x2]


    cv2.rectangle(frame, (x1, y1), (x2, y2), (0,255,0), 2)

    gray = cv2.cvtColor(roi, cv2.COLOR_BGR2GRAY)
    blur = cv2.GaussianBlur(gray, (5,5), 2)
    _, thresh = cv2.threshold(blur, 70, 255, cv2.THRESH_BINARY_INV+cv2.THRESH_OTSU)

    resized = cv2.resize(thresh, (IMG_WIDTH, IMG_HEIGHT))
    normalized = resized.astype('float32') / 255.0
    reshaped = normalized.reshape(1, IMG_WIDTH, IMG_HEIGHT, 1)


    prediction = model.predict(reshaped)
    class_id = np.argmax(prediction)
    gesture_name = gesture_classes[class_id]


    cv2.putText(frame, f"Gesture: {gesture_name}", (x1, y1-40), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255, 0, 0), 2)

    current_time = time.time()
    if gesture_name != last_executed and (current_time - last_time) > cooldown:
        action = gesture_actions.get(gesture_name)
        if action:
            print(f"Gesture: {gesture_name} --> Action executed.")
            action()
            last_executed = gesture_name
            last_time = current_time


    cv2.imshow("Webcam", frame)
    cv2.imshow("ROI", thresh)

    key = cv2.waitKey(1)
    if key == 27:  # Esc
        break

cap.release()
cv2.destroyAllWindows()

